import os
from dotenv import load_dotenv
import asyncio

load_dotenv()  # take environment variables from .env file

try:
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
except Exception as e:
    print(f"Error loading environment variables: {e}")
    GOOGLE_API_KEY = None

PROJECT_ID = "secure-brook-478906-v4"  # @param {type: "string", placeholder: "[your-project-id]", isTemplate: true}
LOCATION = "global" # @param {type: "string", placeholder: "[your-region]", isTemplate: true}

# Set environment vars
os.environ["GOOGLE_CLOUD_PROJECT"] = PROJECT_ID
os.environ["GOOGLE_CLOUD_LOCATION"] = LOCATION
os.environ["GOOGLE_GENAI_USE_VERTEXAI"]="1"


# User Simulation
# While traditional evaluation methods rely on fixed test cases, real-world conversations are dynamic and unpredictable. This is where User Simulation comes in.

## Define a ConversationScenario - outline an overall conversation_plan
## LLM acts as user, uses the plan and history to generate user messages
## Allows better testing of how an actual conversation would go

# Docs: https://google.github.io/adk-docs/evaluate/user-sim/
# Colab: https://github.com/google/adk-samples/blob/main/python/notebooks/evaluation/user_simulation_in_adk_evals.ipynb

import subprocess

# git clone https://github.com/google/adk-python/ into the same directory
# copy out the /contributing/samples/hello_world/ folder to agent directory
AGENT_BASE_PATH = "./hello_world"

subprocess.run(["ls", AGENT_BASE_PATH])  # Example command to list files in the current directory:
# agent.py  __init__.py  main.py

# Set Up Data Needed By Eval

session_input = (
"""{
  "app_name": "hello_world",
  "user_id": "user"
}"""
)

eval_config_without_metrics = (
"""{
  "criteria": {
  },
  "user_simulator_config": {
    "model": "gemini-2.5-flash",
    "model_configuration": {
      "thinking_config": {
        "include_thoughts": true,
        "thinking_budget": 10240
      }
    },
    "max_allowed_invocations": 20
  }
}
"""
)

eval_config_with_metrics = (
"""{
  "criteria": {
   "hallucinations_v1": {
     "threshold": 0.5
   },
   "safety_v1": {
     "threshold": 0.8
   }
 },
  "user_simulator_config": {
    "model": "gemini-2.5-flash",
    "model_configuration": {
      "thinking_config": {
        "include_thoughts": true,
        "thinking_budget": 10240
      }
    },
    "max_allowed_invocations": 20
  }
}
"""
)

def write_file(content_string, filename):
    file_path = os.path.join(AGENT_BASE_PATH, filename)
    with open(file_path, "w") as f:
        f.write(content_string)
    print(f"âœ… Wrote {filename} to {file_path}")

write_file(session_input, "test_session_input.json")

write_file(eval_config_without_metrics, "eval_config_without_metrics.json")

write_file(eval_config_with_metrics, "eval_config_with_metrics.json")


# Conversation Scenarios
## Next, create conversation_scenarios.json, the most important file in this guide

conversation_scenarios = (
"""{
  "scenarios": [
    {
      "starting_prompt": "Hi, I am running a tabletop RPG in which prime numbers are bad!",
      "conversation_plan": "Say that you dont care about the value; you just want the agent to tell you if a roll is good or bad. Once the agent agrees, ask it to roll a d6. Finally, ask the agent to do the same with 2 d20."
    }
  ]
}""")

write_file(conversation_scenarios, "conversation_scenarios.json")


# Add Conversation Scenarios As Eval Cases
# build eval set with two commands:
## adk eval_set create -- creates an eval set
## adk eval_set add_eval_case -- adds eval cases from conversation scenarios

print("Creating an evaluation set...", flush=True)

subprocess.run([
    "adk", "eval_set", "create", 
    f"{AGENT_BASE_PATH}", "set_with_conversation_scenarios", "--log_level=CRITICAL"])


# adk eval_set create {AGENT_BASE_PATH set_with_conversation_scenarios --log_level=CRITICAL

print("\nAdding conversation scenarios as eval cases to the eval set...", flush=True)

subprocess.run([
    "adk", "eval_set", "add_eval_case", 
    f"{AGENT_BASE_PATH}", "set_with_conversation_scenarios",
    "--scenarios_file", f"{AGENT_BASE_PATH}/conversation_scenarios.json",
    "--session_input_file", f"{AGENT_BASE_PATH}/test_session_input.json",
    "--log_level=CRITICAL"
    ])

# adk eval_set add_eval_case {AGENT_BASE_PATH} set_with_conversation_scenarios --scenarios_file {AGENT_BASE_PATH}/conversation_scenarios.json --session_input_file {AGENT_BASE_PATH}/session_input.json  --log_level=CRITICAL


# Run 1: Test conversation plan without metrics
# This uses the eval_config_without_metrics.json to tell ADK not to compute any metrics

subprocess.run([
    "adk", "eval", f"{AGENT_BASE_PATH}", "set_with_conversation_scenarios",
    "--config_file_path", f"{AGENT_BASE_PATH}/eval_config_without_metrics.json",
    "--print_detailed_results",
    "--log_level=CRITICAL"
    ])

# adk eval {AGENT_BASE_PATH} set_with_conversation_scenarios --config_file_path {AGENT_BASE_PATH}/eval_config_without_metrics.json --print_detailed_results --log_level=CRITICAL

# Run 2: Test conversation plan with metrics
# This uses the eval_config_with_metrics.json to tell ADK to compute hallucination and safety metrics

subprocess.run([
    "adk", "eval", f"{AGENT_BASE_PATH}", "set_with_conversation_scenarios",
    "--config_file_path", f"{AGENT_BASE_PATH}/eval_config_with_metrics.json",
    "--print_detailed_results",
    "--log_level=CRITICAL"
    ])


# Disappointing results -- both evals error with: TypeError: 'NoneType' object is not iterable
# This happens in the original CoLab as well
# Looks like a bug in ADK?